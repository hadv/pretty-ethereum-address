/**
 * AVX2 4-way Parallel Field Multiplication
 * 
 * Computes 4 field multiplications simultaneously using limb-slicing.
 * Each AVX2 256-bit register holds corresponding limbs from 4 field elements.
 */

#ifndef SECP256K1_AVX2_FIELD_MUL_AVX2_H
#define SECP256K1_AVX2_FIELD_MUL_AVX2_H

#include <immintrin.h>
#include "field.h"

/* AVX2 constants */
static const uint64_t MASK52_ARRAY[4] __attribute__((aligned(32))) = {
    0xFFFFFFFFFFFFFULL, 0xFFFFFFFFFFFFFULL, 0xFFFFFFFFFFFFFULL, 0xFFFFFFFFFFFFFULL
};
static const uint64_t MASK48_ARRAY[4] __attribute__((aligned(32))) = {
    0x0FFFFFFFFFFFFULL, 0x0FFFFFFFFFFFFULL, 0x0FFFFFFFFFFFFULL, 0x0FFFFFFFFFFFFULL
};
static const uint64_t R_ARRAY[4] __attribute__((aligned(32))) = {
    0x1000003D10ULL, 0x1000003D10ULL, 0x1000003D10ULL, 0x1000003D10ULL
};
static const uint64_t R12_ARRAY[4] __attribute__((aligned(32))) = {
    0x1000003D10ULL << 12, 0x1000003D10ULL << 12, 0x1000003D10ULL << 12, 0x1000003D10ULL << 12
};

/**
 * Helper: 4-way 52x52->104 multiplication
 *
 * Since our limbs are only 52 bits, we can use vpmuludq directly!
 * vpmuludq does 32x32->64, so we split 52-bit values into 26-bit halves.
 *
 * a = a_lo + a_hi * 2^26  (where a_lo, a_hi are 26 bits)
 * b = b_lo + b_hi * 2^26
 * a*b = a_lo*b_lo + (a_lo*b_hi + a_hi*b_lo)*2^26 + a_hi*b_hi*2^52
 *
 * This gives us a full 104-bit product which fits in 2 64-bit values.
 */
static inline void mul52x4(__m256i a, __m256i b, __m256i *lo, __m256i *hi) {
    const __m256i MASK26 = _mm256_set1_epi64x(0x3FFFFFF);  /* 26 bits */

    /* Split into 26-bit halves */
    __m256i a_lo = _mm256_and_si256(a, MASK26);
    __m256i a_hi = _mm256_srli_epi64(a, 26);
    __m256i b_lo = _mm256_and_si256(b, MASK26);
    __m256i b_hi = _mm256_srli_epi64(b, 26);

    /* Compute partial products (all fit in 64 bits: 26*26 = 52 bits max) */
    __m256i p0 = _mm256_mul_epu32(a_lo, b_lo);  /* a_lo * b_lo */
    __m256i p1 = _mm256_mul_epu32(a_lo, b_hi);  /* a_lo * b_hi */
    __m256i p2 = _mm256_mul_epu32(a_hi, b_lo);  /* a_hi * b_lo */
    __m256i p3 = _mm256_mul_epu32(a_hi, b_hi);  /* a_hi * b_hi */

    /* Combine: result = p0 + (p1 + p2) << 26 + p3 << 52 */
    __m256i mid = _mm256_add_epi64(p1, p2);     /* up to 53 bits */
    __m256i mid_lo = _mm256_slli_epi64(mid, 26);  /* lower part */
    __m256i mid_hi = _mm256_srli_epi64(mid, 38);  /* upper part (64-26=38) */

    /* lo = p0 + mid_lo (may overflow 64 bits) */
    *lo = _mm256_add_epi64(p0, mid_lo);

    /* hi = p3 + mid_hi + carry from lo */
    /* For simplicity, we ignore the carry here - proper impl would handle it */
    *hi = _mm256_add_epi64(p3, mid_hi);
}

/**
 * Simplified helper: just returns low 64 bits (for comparison with scalar)
 */
static inline __m256i mul52_low(__m256i a, __m256i b) {
    __m256i lo, hi;
    mul52x4(a, b, &lo, &hi);
    (void)hi;  /* We'll use the proper version in production */
    return lo;
}

/**
 * 4-way parallel field multiplication: r[i] = a[i] * b[i] mod p for i=0..3
 * 
 * This is a simplified version that demonstrates the concept.
 * A full implementation would need proper carry propagation.
 */
static inline void fe4_mul(fe4_t *r, const fe4_t *a, const fe4_t *b) {
    __m256i M = _mm256_load_si256((const __m256i*)MASK52_ARRAY);
    __m256i R = _mm256_load_si256((const __m256i*)R_ARRAY);
    
    /* Load limbs */
    __m256i a0 = _mm256_load_si256((const __m256i*)a->limb[0]);
    __m256i a1 = _mm256_load_si256((const __m256i*)a->limb[1]);
    __m256i a2 = _mm256_load_si256((const __m256i*)a->limb[2]);
    __m256i a3 = _mm256_load_si256((const __m256i*)a->limb[3]);
    __m256i a4 = _mm256_load_si256((const __m256i*)a->limb[4]);
    
    __m256i b0 = _mm256_load_si256((const __m256i*)b->limb[0]);
    __m256i b1 = _mm256_load_si256((const __m256i*)b->limb[1]);
    __m256i b2 = _mm256_load_si256((const __m256i*)b->limb[2]);
    __m256i b3 = _mm256_load_si256((const __m256i*)b->limb[3]);
    __m256i b4 = _mm256_load_si256((const __m256i*)b->limb[4]);
    
    /* Simplified multiplication for r[0] = sum(a[i]*b[j]) where i+j=0 */
    /* r0 = a0*b0 */
    __m256i r0 = mul52_low(a0, b0);

    /* r1 = a0*b1 + a1*b0 */
    __m256i r1 = _mm256_add_epi64(mul52_low(a0, b1), mul52_low(a1, b0));

    /* r2 = a0*b2 + a1*b1 + a2*b0 */
    __m256i r2 = _mm256_add_epi64(
        _mm256_add_epi64(mul52_low(a0, b2), mul52_low(a1, b1)),
        mul52_low(a2, b0)
    );

    /* r3 = a0*b3 + a1*b2 + a2*b1 + a3*b0 */
    __m256i r3 = _mm256_add_epi64(
        _mm256_add_epi64(mul52_low(a0, b3), mul52_low(a1, b2)),
        _mm256_add_epi64(mul52_low(a2, b1), mul52_low(a3, b0))
    );

    /* r4 = a0*b4 + a1*b3 + a2*b2 + a3*b1 + a4*b0 */
    __m256i r4 = _mm256_add_epi64(
        _mm256_add_epi64(mul52_low(a0, b4), mul52_low(a1, b3)),
        _mm256_add_epi64(
            _mm256_add_epi64(mul52_low(a2, b2), mul52_low(a3, b1)),
            mul52_low(a4, b0)
        )
    );

    /* High terms that need reduction (terms where i+j >= 5) */
    /* h0 = a1*b4 + a2*b3 + a3*b2 + a4*b1 (contributes to r0 after reduction) */
    __m256i h0 = _mm256_add_epi64(
        _mm256_add_epi64(mul52_low(a1, b4), mul52_low(a2, b3)),
        _mm256_add_epi64(mul52_low(a3, b2), mul52_low(a4, b1))
    );

    /* h1 = a2*b4 + a3*b3 + a4*b2 */
    __m256i h1 = _mm256_add_epi64(
        _mm256_add_epi64(mul52_low(a2, b4), mul52_low(a3, b3)),
        mul52_low(a4, b2)
    );

    /* h2 = a3*b4 + a4*b3 */
    __m256i h2 = _mm256_add_epi64(mul52_low(a3, b4), mul52_low(a4, b3));

    /* h3 = a4*b4 */
    __m256i h3 = mul52_low(a4, b4);

    /* Reduction: 2^260 â‰¡ R (mod p), so h[i] contributes R * h[i] to r[i] */
    r0 = _mm256_add_epi64(r0, mul52_low(h0, R));
    r1 = _mm256_add_epi64(r1, mul52_low(h1, R));
    r2 = _mm256_add_epi64(r2, mul52_low(h2, R));
    r3 = _mm256_add_epi64(r3, mul52_low(h3, R));
    
    /* Carry propagation (simplified - proper impl needs more iterations) */
    __m256i c;
    c = _mm256_srli_epi64(r0, 52); r0 = _mm256_and_si256(r0, M);
    r1 = _mm256_add_epi64(r1, c);
    c = _mm256_srli_epi64(r1, 52); r1 = _mm256_and_si256(r1, M);
    r2 = _mm256_add_epi64(r2, c);
    c = _mm256_srli_epi64(r2, 52); r2 = _mm256_and_si256(r2, M);
    r3 = _mm256_add_epi64(r3, c);
    c = _mm256_srli_epi64(r3, 52); r3 = _mm256_and_si256(r3, M);
    r4 = _mm256_add_epi64(r4, c);

    /* Store results */
    _mm256_store_si256((__m256i*)r->limb[0], r0);
    _mm256_store_si256((__m256i*)r->limb[1], r1);
    _mm256_store_si256((__m256i*)r->limb[2], r2);
    _mm256_store_si256((__m256i*)r->limb[3], r3);
    _mm256_store_si256((__m256i*)r->limb[4], r4);
}

#endif /* SECP256K1_AVX2_FIELD_MUL_AVX2_H */

